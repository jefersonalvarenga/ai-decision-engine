"""
GLM Caller - Reusable helper to call the GLM-5 API.

Used for code generation, test analysis, and message critique tasks.
The GLM-5 acts as the 'executor' while Claude handles strategy.
"""
import os
import json
import re
import httpx
from dotenv import load_dotenv

load_dotenv()

GLM_API_URL = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
MODEL_NAME = "glm-5"


def _get_api_key() -> str:
    api_key = os.getenv("GLM-API-KEY")
    if not api_key:
        raise RuntimeError("Environment variable 'GLM-API-KEY' is missing or empty.")
    return api_key


def _strip_markdown_fences(text: str) -> str:
    pattern = r'^```(?:\w+)?\s*\n(.*?)\n```\s*$'
    match = re.match(pattern, text, re.DOTALL)
    if match:
        return match.group(1).strip()
    text = re.sub(r'^```(?:\w+)?\s*\n?', '', text)
    text = re.sub(r'\n?```\s*$', '', text)
    return text.strip()


def call_glm(prompt: str, temperature: float = 0.7, max_tokens: int = 6000) -> str:
    """
    Calls the GLM-5 API with the provided prompt.

    Args:
        prompt: The user prompt to send to the model.
        temperature: Sampling temperature.
        max_tokens: Maximum tokens to generate.

    Returns:
        The content string from the model response, with markdown fences stripped.

    Raises:
        RuntimeError: If the API call fails or returns an error.
    """
    api_key = _get_api_key()

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature,
        "max_tokens": max_tokens
    }

    try:
        with httpx.Client(timeout=120.0) as client:
            response = client.post(GLM_API_URL, headers=headers, json=payload)

            if response.status_code != 200:
                error_detail = response.text
                try:
                    error_json = response.json()
                    if "error" in error_json:
                        error_detail = error_json["error"].get("message", str(error_json))
                except json.JSONDecodeError:
                    pass
                raise RuntimeError(f"GLM API Error {response.status_code}: {error_detail}")

            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            return _strip_markdown_fences(content)

    except httpx.RequestError as exc:
        raise RuntimeError(f"An error occurred while requesting the GLM API: {exc}")


def analyze_test_report(test_report: str, agent_code: str, signature_code: str) -> str:
    """
    Sends test results + agent code to GLM-5 for analysis and improvement suggestions.

    Returns:
        GLM-5's full analysis including new test case suggestions and prompt improvements.
    """
    prompt = f"""Você é um especialista em agentes de IA para vendas B2B via WhatsApp em PT-BR.

Analise este relatório de testes do Gatekeeper SDR Agent (agente que contata recepções de clínicas
para obter o WhatsApp do gestor) e sugira melhorias.

=== RELATÓRIO DE TESTES ===
{test_report}

=== CÓDIGO DO AGENTE ===
{agent_code}

=== SIGNATURE/PROMPT DO AGENTE ===
{signature_code}

Por favor forneça:

1. **ANÁLISE DAS FALHAS**: Para cada cenário que falhou, explique a causa raiz.

2. **MELHORIAS NO PROMPT**: Sugestões específicas e concretas para a signature do agente
   que corrijam as falhas identificadas.

3. **10 NOVOS CENÁRIOS DE TESTE** em JSON válido:
   Cada cenário no formato:
   {{"name": "...", "clinic_name": "...", "conversation_history": [...], "latest_message": "...", "expected_stage": "..."}}

   Cubra estes gaps: pedido de PDF, número com formatação especial, retorno amanhã,
   recepção agressiva, transferência + número junto, confusão sobre qual gestor,
   recepcionista é o gestor, pede nome da empresa, clinic_name com acentos, recusa definitiva.

Seja específico e prático."""

    return call_glm(prompt, temperature=0.5, max_tokens=4000)


def critique_message(message: str, stage: str, clinic_name: str, latest_reception_msg: str) -> dict:
    """
    Critiques a WhatsApp message generated by the Gatekeeper agent.

    Args:
        message: The message to evaluate.
        stage: Current conversation stage.
        clinic_name: Clinic context.
        latest_reception_msg: Last message from receptionist.

    Returns:
        dict with keys: approved (bool), feedback (str), suggested_message (str|None)
    """
    prompt = f"""Você é um especialista em comunicação B2B via WhatsApp.

Avalie esta mensagem gerada por um SDR para enviar à recepção de uma clínica:

CLÍNICA: {clinic_name}
STAGE DA CONVERSA: {stage}
ÚLTIMA MENSAGEM DA RECEPÇÃO: "{latest_reception_msg}"
MENSAGEM GERADA PELO AGENTE: "{message}"

Critérios de aprovação:
- Natural em PT-BR (não robótica)
- Curta (máximo 1-2 frases, ~100 caracteres)
- Sem emojis
- Apropriada para o stage atual
- Estrategicamente correta (avança em direção ao objetivo: obter contato do gestor)

Responda APENAS com JSON válido neste formato exato:
{{"approved": true/false, "feedback": "explicação", "suggested_message": "mensagem alternativa ou null"}}"""

    raw = call_glm(prompt, temperature=0.3, max_tokens=500)

    # Parse JSON from response
    try:
        # Try direct parse
        return json.loads(raw)
    except json.JSONDecodeError:
        # Try to extract JSON from text
        match = re.search(r'\{.*\}', raw, re.DOTALL)
        if match:
            try:
                return json.loads(match.group())
            except json.JSONDecodeError:
                pass
    # Fallback
    return {"approved": True, "feedback": "Could not parse GLM response", "suggested_message": None}
